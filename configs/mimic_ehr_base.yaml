# JEPA-EHR Configuration
# Adapted from I-JEPA for Electronic Health Records (MIMIC-IV)

data:
  # Path to MIMIC-IV processed data
  data_path: /Users/abelyagubyan/Downloads/EHRJEPA/data/processed_mimic/mimic_hourly_sequences.csv
  
  # Batch size - adjust based on GPU memory
  batch_size: 32
  
  # Sequence parameters
  sequence_length: 20  # Total number of visits in sequence
  context_length: 15   # Number of visits used as context
  prediction_length: 5 # Number of visits to predict
  
  # Feature columns (null means use all available features)
  # Specify feature names if you want to select specific columns
  feature_columns: null
  
  # Data loader settings
  num_workers: 4
  pin_mem: true
  
  # Whether to drop last incomplete batch
  drop_last: true

logging:
  # Directory for experiment logs and checkpoints
  folder: /Users/abelyagubyan/Downloads/EHRJEPA/logs/jepa_ehr_base/
  
  # Tag for this experiment run
  write_tag: jepa-ehr

mask:
  # Masking strategy for temporal sequences
  # Options: 'simple' (fixed future prediction) or 'temporal' (block sampling)
  masking_strategy: simple
  
  # For 'temporal' masking strategy
  allow_overlap: false
  num_context_blocks: 1
  num_pred_blocks: 1
  block_size_range: [1, 5]  # Min and max size of temporal blocks
  context_ratio: 0.75       # Ratio of sequence to use as context

meta:
  # Model architecture
  # Options: temporal_transformer_tiny, temporal_transformer_small, 
  #          temporal_transformer_base, temporal_transformer_large
  model_name: temporal_transformer_base
  
  # Predictor configuration
  pred_depth: 6           # Number of transformer layers in predictor
  pred_emb_dim: 384       # Embedding dimension in predictor
  
  # Checkpoint settings
  load_checkpoint: false
  read_checkpoint: null   # Path to checkpoint file (if load_checkpoint=true)
  
  # Precision
  use_bfloat16: true      # Use bfloat16 for training (faster on modern GPUs)

optimization:
  # Training epochs
  epochs: 20
  
  # Learning rate schedule
  start_lr: 0.0001        # Warmup starting learning rate
  lr: 0.0005              # Peak learning rate
  final_lr: 1.0e-06       # Final learning rate after decay
  warmup: 10              # Warmup epochs
  
  # Weight decay (L2 regularization)
  weight_decay: 0.04
  final_weight_decay: 0.4
  
  # EMA (Exponential Moving Average) for target encoder
  ema: [0.996, 1.0]       # [start_ema, end_ema]
  
  # Iterations per epoch scale (for scheduler)
  ipe_scale: 1.0
